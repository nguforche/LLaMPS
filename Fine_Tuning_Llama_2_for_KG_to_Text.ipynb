{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguforche/LLaMPS/blob/main/Fine_Tuning_Llama_2_for_KG_to_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KG-to-Text with Llama 2\n",
        "This notebook fine-tunes Llama 2 using QLoRA for the KG-to-Text. The model is fine-tuned on the WebNLG (Constrained) training set."
      ],
      "metadata": {
        "id": "FbhoqBIidpe4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n",
        "This notebook uses the version 2.1 constrained dataset, which contains three JSON files, each corresponding to a subset of the data (train, dev, and test).\n",
        "The following command clones the WebNLG repo."
      ],
      "metadata": {
        "id": "bLAQRVW9blJc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J334xE50vQW"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "git clone https://gitlab.com/shimorina/webnlg-dataset.git\n",
        "ls webnlg-dataset/release_v2.1_constrained/json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompting\n",
        "The prompt template is defined according to the [Llama 2 chat prompt template standard](https://huggingface.co/blog/llama2).\n",
        "The formatted triples will replace the `{triples}` placeholder."
      ],
      "metadata": {
        "id": "3wKp6qw6b34r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text = \"\"\"<s>[INST] Following is a set of knowledge graph triples delimited by triple backticks, each on a separate line, in the format: subject | predicate | object.\n",
        "\n",
        "```\n",
        "{triples}\n",
        "```\n",
        "\n",
        "Generate a coherent piece of text that contains all of the information in the triples. Only use information from the provided triples.\n",
        "After you finish writing the piece of text, write triple dollar signs (i.e.: $$$).[/INST]\"\"\""
      ],
      "metadata": {
        "id": "8NAJvIScbjKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Hugging Face Dataset\n",
        "The Hugging Face dataset loads data from JSON Lines files, so the data should be in this format. The following code creates these files and uses them to create the Hugging Face datasets."
      ],
      "metadata": {
        "id": "BQB9yXEbcVwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets jsonlines\n",
        "\n",
        "import json\n",
        "import jsonlines\n",
        "import os\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "def format_triplets(triplets):\n",
        "        \"\"\"Helper function to format triples.\"\"\"\n",
        "        return '\\n'.join([f\"{triplet['subject']} | {triplet['property']} | {triplet['object']}\" for triplet in triplets])\n",
        "\n",
        "dataset_dir_path = \"webnlg-dataset/release_v2.1_constrained/json\"\n",
        "data_subsets_file_names = {\n",
        "    \"train\": \"webnlg_release_v2.1_constrained_train.json\",\n",
        "    \"dev\": \"webnlg_release_v2.1_constrained_dev.json\",\n",
        "    \"test\": \"webnlg_release_v2.1_constrained_test.json\"\n",
        "}\n",
        "\n",
        "data_subsets_file_paths = {k: os.path.join(dataset_dir_path, v) for k, v in data_subsets_file_names.items()}\n",
        "\n",
        "for data_subset_name, data_subset_file_path in data_subsets_file_paths.items():\n",
        "    all_responses = []\n",
        "    with open(data_subset_file_path, 'r') as data_subset_file:\n",
        "        data_subset_dict = json.load(data_subset_file)\n",
        "\n",
        "    with jsonlines.open(f\"{data_subset_name}.jsonl\", mode='w') as writer:\n",
        "        for i, entry in enumerate(tqdm(data_subset_dict[\"entries\"])):\n",
        "            triples = format_triplets(entry[str(i+1)]['modifiedtripleset'])\n",
        "            responses = [l[\"lex\"] for l in entry[str(i+1)]['lexicalisations']]\n",
        "            all_responses.append(responses)\n",
        "            lexicalizations = entry[str(i+1)]['lexicalisations']\n",
        "            good_responses = [l[\"lex\"] for l in lexicalizations if l[\"comment\"] == \"good\"] \\\n",
        "                if data_subset_name != \"test\" else [lexicalizations[0][\"lex\"]]\n",
        "            for response in good_responses:\n",
        "                prompt = prompt_text.format(triples=triples)\n",
        "                writer.write({\"prompt\": prompt, \"response\": response + \"$$$ </s>\"})\n",
        "\n",
        "# Load the Hugging Face datasets\n",
        "train_dataset = load_dataset('json', data_files='train.jsonl', split=\"train\")\n",
        "\n",
        "# Preprocess the Hugging Face datasets\n",
        "train_dataset = train_dataset.map(\n",
        "    lambda examples: {'text': [f\"{prompt} {response}\"\n",
        "    for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True\n",
        "    )\n",
        "\n",
        "test_dataset = load_dataset('json', data_files='test.jsonl', split=\"train\")"
      ],
      "metadata": {
        "id": "9SOPGXL_cWuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling\n",
        "\n",
        "To use Llama 2, you need to request access to the model from Meta.\n",
        "The instructions to obtain access can be found in the [Llama 2 organization card on Hugging Face](https://huggingface.co/meta-llama).\n",
        "\n",
        "Once you obtain access to the model, follow the following instructions:\n",
        "1. [Obtain your Hugging Face user access token](https://huggingface.co/docs/hub/security-tokens)\n",
        "2. Create a `credentials.env` file in your current working directory. The content of the file should be as follows:\n",
        "```\n",
        "HUGGINGFACE_TOKEN=\"<your_hugging_face_token>\"\n",
        "```\n",
        "3. Run the following code to login to Hugging Face with your token."
      ],
      "metadata": {
        "id": "7Y4N6C7fciVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "\n",
        "from dotenv import dotenv_values\n",
        "\n",
        "config = dotenv_values(\"credentials.env\")\n",
        "HUGGINGFACE_CLI_TOKEN = config[\"HUGGINGFACE_TOKEN\"]\n",
        "\n",
        "!huggingface-cli login --token $HUGGINGFACE_CLI_TOKEN"
      ],
      "metadata": {
        "id": "sp4WNFJtgbdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The following packages are used in the fine-tuning process:\n",
        "- [transformers](https://huggingface.co/docs/transformers/index): The Hugging Face library, used to obtain the Llama 2 7B Chat model.\n",
        "- [peft](https://huggingface.co/docs/peft/index): Short for Parameter-Efficient Fine-Tuning, used to fine-tune the LLM without needing to modify all the model parameters.\n",
        "- [bitsandbytes](https://github.com/TimDettmers/bitsandbytes): This library runs the model in 4-bit precision.\n",
        "- [trl](https://huggingface.co/docs/trl/index): Short for Transformer Reinforcement Learning, the library that does the fine-tuning.\n",
        "\n",
        "The following code defines the model configurations and runs the fine-tuning process.\n",
        "\n",
        "**WARNING**: The following code takes more than 24 hours to run on a T4 GPU."
      ],
      "metadata": {
        "id": "X7MabgVljRgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
        "\n",
        "import torch\n",
        "\n",
        "from peft import LoraConfig\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "output_dir = \"./results_chat_7b_3_epoch/final_checkpoint\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "device_map = {\"\": 0}"
      ],
      "metadata": {
        "id": "FQBDn2-h69eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(  ## If it fails at this line, restart the runtime and try again.\n",
        "    base_model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map,\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=True\n",
        ")\n",
        "base_model.config.use_cache = False\n",
        "\n",
        "# More info: https://github.com/huggingface/transformers/pull/24906\n",
        "base_model.config.pretraining_tp = 1\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=25,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    logging_steps=10,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=base_model,\n",
        "    train_dataset=train_dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.model.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "pH8f4HclcwaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "To obtain the results, the following code creates a Hugging Face pipeline and provides the model and stopping criteria to stop the generation when the triple dollar signs are generated.\n",
        "\n",
        "If the following code produces a \"CUDA out of memory\" error, restart runtime and run all the cells **except for the previous cell**, then run the following cells again."
      ],
      "metadata": {
        "id": "OQJerN3dc0hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import pipeline, StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "class StopOnTripleDollarSigns(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor,\n",
        "                 **kwargs) -> bool:\n",
        "        if ''.join(tokenizer.convert_ids_to_tokens(input_ids[0][-3:])).endswith(\"$$$\"):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=device_map, torch_dtype=torch.bfloat16, load_in_8bit=True)\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTripleDollarSigns()])\n",
        "text_gen = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500, stopping_criteria=stopping_criteria)\n",
        "\n",
        "text_gen = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500, stopping_criteria=stopping_criteria)"
      ],
      "metadata": {
        "id": "ZL6diCOjdAV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the following code generates the text, cleans it by removing the original prompt and the triple dollar signs, and saves it along with the reference texts for evaluation."
      ],
      "metadata": {
        "id": "W0l3wRlJdEuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "\n",
        "def clean_result_text(result_text, test_prompt):\n",
        "    result_text_without_prompt = result_text[len(test_prompt):]\n",
        "    delimiter_index = result_text_without_prompt.find('$$$')\n",
        "    cleaned_result_text = result_text_without_prompt[:delimiter_index].strip()\n",
        "    return cleaned_result_text\n",
        "\n",
        "results = []\n",
        "for out in tqdm(text_gen(KeyDataset(test_dataset, \"prompt\"), top_k=1)):\n",
        "    results.append(out)\n",
        "\n",
        "results_texts = [result[0][\"generated_text\"] for result in results]\n",
        "\n",
        "cleaned_results_texts = [clean_result_text(result_text, test_prompt)\n",
        "    for result_text, test_prompt in zip(results_texts, test_dataset[\"prompt\"])]\n",
        "\n",
        "results_file_text = '\\n'.join(cleaned_results_texts)\n",
        "references_file_text = '\\n\\n'.join(['\\n'.join(responses) for responses in all_responses])\n",
        "\n",
        "with open('results.txt', 'w') as results_file:\n",
        "    results_file.write(results_file_text)\n",
        "\n",
        "with open('references.txt', 'w') as references_file:\n",
        "    references_file.write(references_file_text)"
      ],
      "metadata": {
        "id": "Hje1UvCOdFG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "To evaluate the results, we use [Data-to-text-Evaluation-Metric](https://github.com/wenhuchen/Data-to-text-Evaluation-Metric/) (the library used by JointGT).\n",
        "To run the evaluation process:\n",
        "1. Clone this repo on your local machine and install the required software.\n",
        "2. Download the `results.txt` and `references.txt` files produced from the previous cell to your local machine.\n",
        "3. Change your working directory to the location of the repo on your device and run the `measure_scores.py` script.\n",
        "\n",
        "```bash\n",
        "cd Data-to-text-Evaluation-Metric/\n",
        "python measure_scores.py <path_to_references_file> <path_to_results_file>\n",
        "```\n",
        "\n",
        "Replace `<path_to_references_file>` and `<path_to_results_file>` with the paths to the references and results file we created earlier, respectively."
      ],
      "metadata": {
        "id": "Eg0VohxjdXwS"
      }
    }
  ]
}