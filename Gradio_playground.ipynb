{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguforche/LLaMPS/blob/main/Gradio_playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MczMa8SWEyv-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/drive/My Drive/your-model/\" \"/content/your-model/\""
      ],
      "metadata": {
        "id": "jIoYLOnA_nG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cot9Eixe1bd1"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptDFEhL1CIqV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "if major_version >= 8:\n",
        "    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
        "    !pip install \"unsloth[colab-ampere] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "else:\n",
        "    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
        "    !pip install \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHymlZ2-CAci"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"/content/your-model/\",\n",
        "    max_seq_length = 8192,\n",
        "    dtype = None,\n",
        "    load_in_4bit = False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LznZr5T_B01O"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import transformers\n",
        "import torch\n",
        "from threading import Thread\n",
        "from gradio.themes.utils.colors import Color\n",
        "\n",
        "text_color = \"#FFFFFF\"\n",
        "app_background = \"#0A0A0A\"\n",
        "user_inputs_background = \"#193C4C\"\n",
        "widget_bg = \"#000100\"\n",
        "button_bg = \"#141414\"\n",
        "\n",
        "DESCRIPTION = \"\"\"\n",
        "# chat\n",
        "\"\"\"\n",
        "\n",
        "def chat(user_input, history):\n",
        "    messages = []\n",
        "    for pair in history:\n",
        "        messages.append({'role': 'user', 'content': pair[0]})\n",
        "        messages.append({'role': 'assistant', 'content': pair[1]})\n",
        "    messages.append({'role': 'user', 'content': user_input})\n",
        "\n",
        "    encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "    device = 'cuda'\n",
        "    model_inputs = {'input_ids': encodeds.to(device)}\n",
        "\n",
        "    streamer = transformers.TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    generate_kwargs = dict(\n",
        "        model_inputs,\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=8192,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "        #top_p=0.8,\n",
        "        #temperature=0.85,\n",
        "        #top_k=50\n",
        "    )\n",
        "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
        "    t.start()\n",
        "\n",
        "    model_output = \"\"\n",
        "    for new_text in streamer:\n",
        "        model_output += new_text\n",
        "        yield model_output\n",
        "    return model_output\n",
        "\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Monochrome(font=[gr.themes.GoogleFont(\"Roboto\"), \"Arial\", \"sans-serif\"])) as demo:  #main.\n",
        "    gr.Markdown(DESCRIPTION)\n",
        "    chatbot = gr.ChatInterface(fn=chat)\n",
        "\n",
        "demo.queue()\n",
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}